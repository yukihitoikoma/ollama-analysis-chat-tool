"""CSV Data Analysis Chat Application.

This application allows users to upload CSV files and ask questions about
the data through a chat interface. It uses LLM models for natural language
processing and provides both textual and visual responses.
"""

import os
import logging
from datetime import datetime

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import file storage utilities
from utils.file_storage import (
    save_file_with_timestamp,
    save_chat_session,
    save_analysis_results,
    save_visualization
)

# Import LLM handler utilities
from utils.llm_handler import (
    get_llm_response,
    create_data_summary,
    detect_analysis_request,
    format_analysis_response,
    get_analysis_interpretation,
    get_required_graphs
)

# Import analysis utilities
from analysis.data_analyzer import DataAnalyzer
from analysis.visualization import (
    create_scatter_plot,
    create_correlation_heatmap,
    create_histogram,
    create_regression_plot,
    create_feature_importance_plot,
    create_clustering_plot,
    save_figure_to_file
)

# Import model utilities
from models.ml_model import (
    perform_statistical_analysis,
    prepare_data_for_modeling,
    create_advanced_ml_model,
    save_model
)

# Set page config
st.set_page_config(
    page_title="CSVãƒ‡ãƒ¼ã‚¿åˆ†æãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª",
    page_icon="ğŸ“Š",
    layout="wide"
)

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "df" not in st.session_state:
    st.session_state.df = None
if "model" not in st.session_state:
    st.session_state.model = "ollama"  # Default to Ollama
if "visualizations" not in st.session_state:
    st.session_state.visualizations = []

# Title
st.title("ğŸ“Š CSVãƒ‡ãƒ¼ã‚¿åˆ†æãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª")

# Model selection
model_choice = st.radio(
    "LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:",
    ("Ollama", "Claude"),
    index=0 if st.session_state.model == "ollama" else 1
)
st.session_state.model = "ollama" if model_choice.startswith("Ollama") else "claude"

# File uploader
uploaded_file = st.file_uploader(
    "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
    type=["csv"]
)


def create_and_save_visualization(df, visualization_type="auto", analysis_results=None):
    """ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å¯è¦–åŒ–ã‚’ä½œæˆã—ä¿å­˜ã™ã‚‹

    Args:
        df (pandas.DataFrame): å¯è¦–åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
        visualization_type (str): å¯è¦–åŒ–ã®ã‚¿ã‚¤ãƒ—
        analysis_results (dict): åˆ†æçµæœï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        tuple: (figure, file_path) ä½œæˆã•ã‚ŒãŸå›³ã¨ä¿å­˜å…ˆãƒ‘ã‚¹
    """
    try:
        fig = None

        # å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        if visualization_type == "scatter":
            fig = create_scatter_plot(df)
        elif visualization_type == "correlation":
            fig = create_correlation_heatmap(df)
        elif visualization_type == "histogram":
            fig = create_histogram(df)
        elif visualization_type == "regression" and analysis_results:
            # å›å¸°åˆ†æçµæœãŒã‚ã‚‹å ´åˆ
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if len(numeric_cols) >= 2:
                fig = create_regression_plot(
                    df,
                    numeric_cols[0],
                    numeric_cols[1],
                    predictions=analysis_results.get("predictions")
                )
        elif visualization_type == "feature_importance" and analysis_results:
            # ç‰¹å¾´é‡é‡è¦åº¦ãŒã‚ã‚‹å ´åˆ
            if "feature_importance" in analysis_results and "feature_names" in analysis_results:
                fig = create_feature_importance_plot(
                    analysis_results["feature_names"],
                    analysis_results["feature_importance"]
                )
        elif visualization_type == "clustering" and analysis_results:
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœãŒã‚ã‚‹å ´åˆ
            if "cluster_labels" in analysis_results:
                fig = create_clustering_plot(df, analysis_results["cluster_labels"])
        else:
            # è‡ªå‹•é¸æŠï¼šç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’å„ªå…ˆ
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if len(numeric_cols) >= 2:
                fig = create_correlation_heatmap(df)
            elif len(numeric_cols) == 1:
                fig = create_histogram(df)

        if fig is None:
            return None, None

        # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        folder_path = os.path.join("data", datetime.now().strftime("%Y-%m-%d"))
        file_path = save_figure_to_file(fig, folder_path)

        return fig, file_path

    except Exception as e:
        st.error(f"å¯è¦–åŒ–ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None, None


def display_file_info(df, uploaded_file):
    """Display file information section.

    Args:
        df (pandas.DataFrame): The loaded data.
        uploaded_file: Streamlit file uploader object.
    """
    st.subheader("ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±")
    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {uploaded_file.name}")
    st.write(f"ã‚µã‚¤ã‚º: {uploaded_file.size} ãƒã‚¤ãƒˆ")
    st.write(f"è¡Œæ•°: {len(df)}")
    st.write(f"åˆ—æ•°: {len(df.columns)}")


def display_column_info(df):
    """Display column information section.

    Args:
        df (pandas.DataFrame): The loaded data.
    """
    st.subheader("åˆ—æƒ…å ±")
    dtypes_df = df.dtypes.reset_index()
    dtypes_df.columns = ['åˆ—å', 'ãƒ‡ãƒ¼ã‚¿å‹']
    dtypes_df['ãƒ‡ãƒ¼ã‚¿å‹'] = dtypes_df['ãƒ‡ãƒ¼ã‚¿å‹'].astype(str)
    st.dataframe(dtypes_df)


def display_data_preview(df):
    """Display data preview section.

    Args:
        df (pandas.DataFrame): The loaded data.
    """
    st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    st.dataframe(df.head(10))


def display_basic_statistics(df):
    """Display basic statistics section.

    Args:
        df (pandas.DataFrame): The loaded data.
    """
    st.subheader("åŸºæœ¬çµ±è¨ˆæƒ…å ±")
    st.dataframe(df.describe())


def display_missing_values(df):
    """Display missing values section.

    Args:
        df (pandas.DataFrame): The loaded data.
    """
    st.subheader("æ¬ æå€¤")
    missing_data = df.isnull().sum()
    st.dataframe(missing_data[missing_data > 0])


def handle_chat_interaction(df, uploaded_file):
    """ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†ã™ã‚‹

    Args:
        df (pandas.DataFrame): èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        uploaded_file: Streamlitãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    st.subheader("ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹")

    # ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    for idx, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            # å˜ä¸€ã®ç”»åƒãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            if "image" in message:
                st.plotly_chart(message["image"], use_container_width=True, key=f"chart_history_{idx}")
            if "image_path" in message and message["image_path"]:
                try:
                    st.image(message["image_path"], caption="ä¿å­˜ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ç”»åƒ", use_container_width=True)
                except Exception as img_error:
                    logger.warning(f"ä¿å­˜ç”»åƒã®è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(img_error)}")

            # è¤‡æ•°ã®ç”»åƒãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
            if "images" in message and message["images"]:
                st.markdown("---")
                st.markdown("### ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•")
                for fig_idx, fig in enumerate(message["images"]):
                    try:
                        st.plotly_chart(fig, use_container_width=True, key=f"chart_history_{idx}_{fig_idx}")
                    except Exception as e:
                        logger.warning(f"å±¥æ­´ã‚°ãƒ©ãƒ•è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(e)}")

                # è¤‡æ•°ã®ä¿å­˜ç”»åƒãƒ‘ã‚¹ãŒã‚ã‚‹å ´åˆ
                if "image_paths" in message and message["image_paths"]:
                    for path_idx, img_path in enumerate(message["image_paths"]):
                        try:
                            if os.path.exists(img_path):
                                st.image(img_path, caption=f"ã‚°ãƒ©ãƒ• {path_idx + 1}", use_container_width=True)
                        except Exception as e:
                            logger.warning(f"å±¥æ­´ç”»åƒè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(e)}")

    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if prompt := st.chat_input("ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        with st.chat_message("user"):
            st.markdown(prompt)

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’å‡¦ç†
        with st.chat_message("assistant"):
            with st.spinner("åˆ†æä¸­..."):
                # ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
                data_summary = create_data_summary(df)

                # åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ¤œå‡º
                analysis_info = detect_analysis_request(prompt)

                # åˆ†æçµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
                analysis_results = {}
                analysis_text = ""

                # çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œ
                if analysis_info["statistical_analysis"]:
                    try:
                        stat_results = perform_statistical_analysis(
                            df,
                            analysis_info["statistical_analysis"]
                        )

                        if "error" not in stat_results:
                            # åˆ†æçµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
                            if analysis_info["statistical_analysis"] == "logistic_regression":
                                analysis_text += "\n\n### ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æçµæœ\n\n"
                                analysis_text += f"- **ç²¾åº¦ (Accuracy):** {stat_results['metrics']['accuracy']:.4f}\n"
                                if stat_results.get("feature_importance") is not None:
                                    analysis_results["feature_importance"] = stat_results["feature_importance"]
                                    analysis_results["feature_names"] = [f"ç‰¹å¾´é‡ {i}" for i in range(len(stat_results["feature_importance"]))]

                                    # ç‰¹å¾´é‡ä¿‚æ•°ã®è¡¨ç¤º
                                    analysis_text += "\n**ç‰¹å¾´é‡ä¿‚æ•°ï¼ˆé‡ã¿ï¼‰:**\n\n"
                                    for i, coef in enumerate(stat_results["feature_importance"]):
                                        analysis_text += f"{i+1}. ç‰¹å¾´é‡ {i}: {coef:.6f}\n"

                            elif analysis_info["statistical_analysis"] == "linear_regression":
                                analysis_text += "\n\n### é‡å›å¸°åˆ†æçµæœ\n\n"
                                analysis_text += f"- **å¹³å‡äºŒä¹—èª¤å·® (MSE):** {stat_results['metrics']['mse']:.4f}\n"
                                analysis_text += f"- **å¹³æ–¹æ ¹å¹³å‡äºŒä¹—èª¤å·® (RMSE):** {stat_results['metrics']['rmse']:.4f}\n"
                                analysis_text += f"- **æ±ºå®šä¿‚æ•° (RÂ²):** {stat_results['metrics']['r2']:.4f}\n"
                                if stat_results.get("feature_importance") is not None:
                                    analysis_results["feature_importance"] = stat_results["feature_importance"]
                                    analysis_results["feature_names"] = [f"ç‰¹å¾´é‡ {i}" for i in range(len(stat_results["feature_importance"]))]

                                    # ç‰¹å¾´é‡ä¿‚æ•°ã®è¡¨ç¤º
                                    analysis_text += "\n**ç‰¹å¾´é‡ä¿‚æ•°ï¼ˆé‡ã¿ï¼‰:**\n\n"
                                    for i, coef in enumerate(stat_results["feature_importance"]):
                                        analysis_text += f"{i+1}. ç‰¹å¾´é‡ {i}: {coef:.6f}\n"

                            elif analysis_info["statistical_analysis"] == "association_analysis":
                                analysis_text += "\n\n### é–¢é€£æ€§åˆ†æçµæœ\n\n"
                                analysis_text += "**å¼·ã„ç›¸é–¢é–¢ä¿‚ (|r| > 0.7):**\n\n"
                                for corr in stat_results.get("strong_correlations", []):
                                    analysis_text += f"- {corr['feature1']} ã¨ {corr['feature2']}: {corr['correlation']:.4f}\n"

                            elif analysis_info["statistical_analysis"] == "clustering":
                                analysis_text += "\n\n### ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æçµæœ\n\n"
                                analysis_text += f"- **ã‚¯ãƒ©ã‚¹ã‚¿æ•°:** {stat_results['n_clusters']}\n"
                                analysis_text += f"- **ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢:** {stat_results['silhouette_score']:.4f}\n"
                                analysis_results["cluster_labels"] = stat_results["cluster_labels"]

                            analysis_results["statistical_analysis"] = stat_results
                        else:
                            analysis_text += f"\n\nâš ï¸ {stat_results['error']}\n"
                    except Exception as e:
                        analysis_text += f"\n\nâš ï¸ çµ±è¨ˆåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}\n"

                # ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’å®Ÿè¡Œ
                if analysis_info["model_creation"]:
                    try:
                        with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­..."):
                            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                            X, y, feature_names_list, target_encoder, scaler = prepare_data_for_modeling(df)

                            # é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ä½œæˆ
                            model, metrics, feature_importance, feature_names = create_advanced_ml_model(
                                X, y,
                                model_type="xgboost",
                                target_encoder=target_encoder,
                                feature_selection_method="boruta",
                                hyperparameter_tuning=True
                            )

                            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            model_path = f"models/model_{timestamp}.pkl"
                            os.makedirs("models", exist_ok=True)

                            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
                            model_params = model.get_params() if hasattr(model, 'get_params') else {}

                            model_info = {
                                "model_type": "xgboost",
                                "metrics": metrics,
                                "feature_importance": feature_importance.tolist() if feature_importance is not None else None,
                                "feature_names": feature_names,
                                "model_params": model_params,
                                "timestamp": timestamp
                            }
                            save_model(model, model_path, model_info)

                            # çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
                            analysis_text += "\n\n### äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ä½œæˆçµæœ\n\n"
                            analysis_text += f"- **ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—:** XGBoost\n"
                            analysis_text += f"- **ç‰¹å¾´é‡é¸æŠ:** Boruta\n"
                            analysis_text += f"- **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°:** æœ‰åŠ¹\n"
                            analysis_text += f"- **ãƒ¢ãƒ‡ãƒ«ä¿å­˜å ´æ‰€:** `{model_path}`\n\n"

                            # æœ€é©åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¡¨ç¤º
                            analysis_text += "**æœ€é©åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**\n\n"
                            important_params = ['learning_rate', 'max_depth', 'n_estimators', 'subsample', 'colsample_bytree']
                            for param in important_params:
                                if param in model_params:
                                    analysis_text += f"- **{param}:** {model_params[param]}\n"

                            analysis_text += "\n**è©•ä¾¡æŒ‡æ¨™:**\n\n"
                            for metric, value in metrics.items():
                                analysis_text += f"- **{metric}:** {value:.4f}\n"

                            if feature_importance is not None:
                                analysis_text += "\n**ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆå…¨ç‰¹å¾´é‡ï¼‰:**\n\n"
                                # ã™ã¹ã¦ã®ç‰¹å¾´é‡ã‚’é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
                                sorted_idx = np.argsort(feature_importance)[::-1]
                                for i, idx in enumerate(sorted_idx):
                                    fname = feature_names[idx] if idx < len(feature_names) else f"ç‰¹å¾´é‡ {idx}"
                                    analysis_text += f"{i+1}. {fname}: {feature_importance[idx]:.6f}\n"

                            # ç‰¹å¾´é‡é‡è¦åº¦ã‚’ã‚°ãƒ©ãƒ•ç”¨ã«ä¿å­˜
                            if feature_importance is not None:
                                analysis_results["feature_importance"] = feature_importance
                                analysis_results["feature_names"] = feature_names
                                analysis_results["model_params"] = model_params

                    except Exception as e:
                        analysis_text += f"\n\nâš ï¸ ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}\n"

                # LLMå¿œç­”ã‚’å–å¾—
                llm_response = get_llm_response(prompt, data_summary, st.session_state.model)

                # å®Œå…¨ãªå¿œç­”ã‚’æ§‹ç¯‰
                full_response = llm_response + analysis_text

                # åˆ†æçµæœãŒã‚ã‚‹å ´åˆã¯ã€LLMã«ã‚ˆã‚‹è§£é‡ˆã‚’è¿½åŠ 
                if analysis_text.strip():
                    with st.spinner("åˆ†æçµæœã‚’è§£é‡ˆä¸­..."):
                        interpretation = get_analysis_interpretation(
                            prompt,
                            data_summary,
                            analysis_text,
                            st.session_state.model
                        )
                        if interpretation:
                            full_response += "\n\n---\n\n"
                            full_response += "### ğŸ“Š åˆ†æçµæœã®è§£é‡ˆã¨ææ¡ˆ\n\n"
                            full_response += interpretation

            # å¿œç­”ã‚’è¡¨ç¤º
            st.markdown(full_response)

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¸€æ—¦ä¿å­˜ï¼ˆã‚°ãƒ©ãƒ•ãªã—ï¼‰
            message_data = {"role": "assistant", "content": full_response}
            st.session_state.messages.append(message_data)

            # ã‚°ãƒ©ãƒ•ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º
            generated_figures = []
            generated_paths = []

            # LLMã«å¿…è¦ãªã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒ—ã‚’åˆ¤æ–­ã•ã›ã‚‹
            required_graphs = get_required_graphs(
                prompt,
                data_summary,
                full_response,
                st.session_state.model
            )

            logger.info(f"ç”Ÿæˆã™ã‚‹ã‚°ãƒ©ãƒ•: {required_graphs if required_graphs else 'ãªã—'}")

            # ã‚°ãƒ©ãƒ•ãŒå¿…è¦ãªå ´åˆã®ã¿ç”Ÿæˆ
            if required_graphs:
                with st.spinner("ğŸ“Š ã‚°ãƒ©ãƒ•ä½œæˆä¸­..."):
                    # å„ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒ—ã‚’ç”Ÿæˆ
                    for idx, viz_type in enumerate(required_graphs):
                        try:
                            logger.info(f"ã‚°ãƒ©ãƒ• {idx + 1}/{len(required_graphs)}: {viz_type} ã‚’ä½œæˆä¸­")

                            # ç‰¹æ®Šãªå¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã®å‡¦ç†
                            if viz_type == "regression" and analysis_info["statistical_analysis"] == "linear_regression":
                                fig, fig_path = create_and_save_visualization(
                                    df,
                                    visualization_type="regression",
                                    analysis_results=analysis_results
                                )
                            elif viz_type == "clustering" and "cluster_labels" in analysis_results:
                                fig, fig_path = create_and_save_visualization(
                                    df,
                                    visualization_type="clustering",
                                    analysis_results=analysis_results
                                )
                            elif viz_type == "feature_importance" and "feature_importance" in analysis_results:
                                fig, fig_path = create_and_save_visualization(
                                    df,
                                    visualization_type="feature_importance",
                                    analysis_results=analysis_results
                                )
                            else:
                                # é€šå¸¸ã®å¯è¦–åŒ–
                                fig, fig_path = create_and_save_visualization(
                                    df,
                                    visualization_type=viz_type,
                                    analysis_results=analysis_results
                                )

                            if fig is not None:
                                generated_figures.append(fig)
                                if fig_path:
                                    generated_paths.append(fig_path)
                                    logger.info(f"ã‚°ãƒ©ãƒ•ä¿å­˜æˆåŠŸ: {fig_path}")
                            else:
                                logger.warning(f"ã‚°ãƒ©ãƒ•ä½œæˆå¤±æ•—: {viz_type}")

                        except Exception as viz_error:
                            logger.error(f"ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼ ({viz_type}): {str(viz_error)}")

            # ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
            if generated_figures:
                st.markdown("---")
                st.markdown("### ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•")

                for idx, fig in enumerate(generated_figures):
                    try:
                        chart_key = f"chart_generated_{len(st.session_state.messages)}_{idx}"
                        st.plotly_chart(fig, use_container_width=True, key=chart_key)
                        logger.info(f"ã‚°ãƒ©ãƒ• {idx + 1} ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ")

                        # ä¿å­˜ã•ã‚ŒãŸç”»åƒã‚‚è¡¨ç¤º
                        if idx < len(generated_paths) and generated_paths[idx]:
                            if os.path.exists(generated_paths[idx]):
                                st.image(
                                    generated_paths[idx],
                                    caption=f"ã‚°ãƒ©ãƒ• {idx + 1}",
                                    use_container_width=True
                                )
                    except Exception as display_error:
                        logger.error(f"ã‚°ãƒ©ãƒ•è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(display_error)}")

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆã‚°ãƒ©ãƒ•æƒ…å ±ã‚’è¿½åŠ ï¼‰
            if generated_figures:
                st.session_state.messages[-1]["images"] = generated_figures
            if generated_paths:
                st.session_state.messages[-1]["image_paths"] = generated_paths

            # å¯è¦–åŒ–ãƒ‘ã‚¹ã‚’è¿½è·¡
            if generated_paths:
                if "visualizations" not in st.session_state:
                    st.session_state.visualizations = []
                st.session_state.visualizations.extend(generated_paths)

    # ä¿å­˜ãƒœã‚¿ãƒ³
    if len(st.session_state.messages) > 0:
        if st.button("ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜"):
            try:
                filename = uploaded_file.name if uploaded_file else "unknown_file.csv"
                saved_file_path = save_chat_session(
                    st.session_state.messages,
                    filename
                )

                if saved_file_path:
                    st.success(f"ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {saved_file_path}")
                else:
                    st.error("ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
            except Exception as e:
                st.error(f"ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")


# Process uploaded file
if uploaded_file is not None:
    try:
        # Read CSV file
        df = pd.read_csv(uploaded_file)
        st.session_state.df = df

        # Save uploaded file to disk only once
        if "saved_file_path" not in st.session_state:
            folder_path = os.path.join(
                "data",
                datetime.now().strftime("%Y-%m-%d")
            )
            os.makedirs(folder_path, exist_ok=True)
            saved_file_path = save_file_with_timestamp(
                folder_path,
                uploaded_file.name,
                uploaded_file.getvalue()
            )
            st.session_state.saved_file_path = saved_file_path
            st.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {saved_file_path}")

        # Display file information sections
        display_file_info(df, uploaded_file)
        display_column_info(df)
        display_data_preview(df)
        display_basic_statistics(df)
        display_missing_values(df)

        # Handle chat interaction
        handle_chat_interaction(df, uploaded_file)

    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
else:
    st.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
